\chapter{Introduction}\label{sec:introduction}
Micro Aerial Vehicles (MAVs) are about to play a major role in tasks like search and rescue, environment monitoring, security surveillance, inspection, goods delivery (Amazon), etc.  However, for such operations, navigating based on GPS information only is not sufficient. Fully autonomous operation in cities or other dense environments requires MAVs to fly at low altitudes where GPS signals are often shadowed or indoors, and to actively explore unknown environments while avoiding collisions and creating maps. Precisely autonomous operations requires MAVs to rely on alternative localization systems. For minimal weight, power consumption and budget a single camera can be used.\\

Real-time monocular Visual Odometry (VO) algorithms can be used to estimate the 6 DoF pose of a camera relative to its surroundings. This is attractive for many applications such as mobile robotics (and not only aerial) and Augmented Reality (AR) because cameras are small and self-contained and therefore easy to attach to autonomous robots or AR displays. Further, they are cheap, and are now often pre-integrated into mobile computing devices such as PDAs, phones and laptops.\\

SVO (Semi-direct Visual Odometry) \cite{Forster2014} is a very fast VO algorithm able to run at more than 300 frames per second on a consumer laptop. It builds a map based on keyframes and salient points. Most monocular VO are feature-based where scale and rotation invariant descriptors (SIFT, SURF\ldots) are extracted and matched in order to recover the motion from frame to frame while finally refining the pose with reprojection error minimization with the map. SVO uses a different approach by using direct methods. Instead of matching descriptors, it uses intensity gradient to minimize the error between patches around detected salient points over the frame to frame transformation. Finally, it uses Bundle Adjustment to align with the map and avoid or minimize derive.\\

The main problem with most existing monocular VO implementations (including SVO) is a lack of robustness. Rapid camera motions, occlusion, and motion blur (phenomena which are common in all but the most constrained experimental settings) can often cause tracking to fail. While this is inconvenient with any tracking system, tracking failure is particularly problematic for VO systems: not only is camera pose lost, but the estimated map could become corrupted as well. \\

This problem is accentuated during a fast  agile maneuver (e.g., a flip) and so a good relocalization is important when these are intended to be performed. \\

\subsection{Objectives}
\label{sub:objectives}

The objective of this work is to investigate and implement different monocular vision-based relocalization algorithms which should work in the following scheme:

\begin{itemize}
  \item In a training stage, the vehicle explores the environment where the relocalization is supposed to occur. During this stage, an appropriate representation of the scene is created. 
  \item Something happens and the vehicle gets lost. For example the vehicle executes an agile maneuver during which vision-based tracking is no feasible.
  \item During the actual relocalization phase, the 6 DoF pose in  the built map must be estimated.
\end{itemize}

\chapter{Background}
\label{cha:background}


\section{Related Work}\label{sec:related_work}

\subsubsection{Place Recognition}
\label{ssub:place_recognition}


Klein and Murray presents in \cite{Klein2008improving} the relocalization method used in PTAM \cite{KleinMurray2007}. PTAM is a VO algorithm based on keyframes that are used during the relocalization. The relocalization method consists of two steps. First, given the current frame, the most similar keyframe is retrieved, and its know pose is used as a baseline. As measure of similarity the difference between subsampled, blurred and zero-mean images is used. This measure is a cross correlation. The small blurry images are storied every time there is a new keyframe and the small blurry image of a new frame is computed during the relocalization to be compared with the keyframes.\\

Other methods can be used for image retrieval, for example using bag of words. Nist\'er and Stewenius \cite{Nister2006} proposes to use a tree structure to store words in order to handle much larger vocabulary or have a much faster retrieval. Every node of the tree would have $k$ child nodes which are the clustering results of $k$-means. The tree would be build by recursive $k$-means.\\

This structure is expensive to build because $k$-means is very resource consuming. During the online process, new words can be appended to the final leaves.\\

\"Ozuysal et al. \cite{Ozuysal2010} proposes a simplified random forest classifier which relates image patches to objects. It is simplified because instead of using a tree structure they use a linear structure applying all the binary tests to the patch. The result of the tests is a binary descriptor, the list of binary tests is called Fern. Every object is trained with multiple random warps of the known view to introduce information from possible different views of the object. In the end every object can be represented with many binary descriptors and every descriptor should output a probability distribution of possible objects represented. Evaluating multiple Ferns and joining the yelled distributions the final classification is achieved.\\

\subsubsection{Pose Estimation}
\label{ssub:pose_estimation}


During the second step of the relocalization of PTAM, the transformation from the retrieved frame is calculated. This transformation will be finally appended to the know keyframe pose. To do so, an image alignment algorithm, Efficient Second-Order Minimization method (ESM) \cite{Benhimane2006}, is employed. ESM is a Gauss-Newton gradient descent algorithm which can be used with different image warp functions, it is a Lucas-Kanade \cite{Baker2004} algorithm that uses Second-order functions; therefore, results in a faster convergence.\\

Geometric methods are typically used to find the transformation from the fond keyframe using the classic pipeline of salient points detection, feature extraction and matching. The 5pt algorithm can then be used to find the 6 DoF transformation or the 3pt algorithm if depth is known.


\subsubsection{Joint Place and Pose estimation}
\label{ssub:joint_place_and_pose_estimaton}

One approach to solve the relocalization problem was proposed by Williams \cite{Williams2007}. In their implementation, they use Random Forest classifiers to characterize a salient object in space. To do so, the classifier needs to be trained with as many as possible representations of the object (multiple views). Therefore, the first time an object is found, multiple warps of the of the patch are used to initialize its presence in the classifier. On later encounters with the object, the classifier is incrementally trained with additional data. During the relocalization phase salient points are classified using the trained classifier and the 3pt algorithm is used to recover the 6 DoF position. This method is is memory expensive and requires of GPU power to generates the patch warps.\\

Shotton et al. \cite{Shotton2013} also propose a method using random forests. RGB-D data is used to train the classifier. In this case, all the information is encoded in the classifier so no previous data storing or computing (salient point detection, descriptor extraction, etc...) is needed. The classifier is trained to an individual RGB-D pixel, and an RGB-D pixel query will output a probability distribution over the position in $\mathbb{R}^3$. This can be applied to all pixels of a frame or to a sparse subset selection of them. Ideally, the camera pose can be inferred from only three pixels, but as the output of the classifier can be very noisy, a second step is applied. From the output from many pixels an energy function is minimized using preemptive RANSAC in order to find a pose that agrees with most of the distributions.\\

To train this method a very complete dataset of RGB-D images with 6 DoF poses from the environment associated to them is needed. That makes it difficult to be used with SLAM problems where the map get populated incrementally. An online training  method should be developed.\\


\section{Methods}
\label{sec:methods}


\subsubsection{Semi-Direct Visual Odometry}
\label{ssub:semi_direct_visual_odometry}

SVO is an algorithm used to track the pose of a camera over time based on keyframes. It is divided in to main parts, on one side there is the creation and maintenance of a map, and on the other side, the motion estimation. \\

The map consists of images, its pose and world frame points. World points are computed from featured points in the image. To find the depth of these points the following depth filter is used. When a point is first observed a possible depth distribution is initialized with a high uncertainty. This distribution is then updated in posterior observations of the same point in a Bayesian fashion. To find a point in a new image the patch around the frame it was found it taken and slid following the epipolar line in the new image. The point whose patch has maximum correlation is taken as valid. Once the uncertainty of the depth distribution is low enough, its accepted in the map and used in the tracking step.\\

Featured points are only computed in keygrames and then tracked in the following frames. A new keyframe will come in after a certain translation.\\

To fond the pose of a new incoming frame the following steps are performed. First, a sparse image alignment over $SE(3)$ is performed between the new frame and the last one. That is, patches from featured points whose depth is know from the last frame are projected into the new frame. The difference between patches is an error function on the transformation that is then minimized using an iterative Gauss-Newton procedure.\\

After this step a global pose of the frame is found. To improve the results not only the last frame should be used. All the points from the map, that are visible to the frame, are then projected on it straight from the keyframe where they were initially found and a similar alignment is performed. In this case, as the used frames might be farther away and the used patch is larger, an affine warp is applied to the patch. \\

Finally, a motion-only Bundle Adjustment is applied to the map to optimize the pose of keyframes and the 3D location of the world points.\\


\subsubsection{Interest point detection, descriptor extraction and matching}
\label{ssub:interest_point_detection_descriptor_extraction_and_matching}







